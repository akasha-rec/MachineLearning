240510

pip install xgboost(앙상블 부스트 알고리즘)
현재는 catboost 사용 多

범CJ계열ㅋㅋ (정석이 제일 좋아)
일반적 + 통계 / 수학 / 자연계열 선호 - 데이터 정제만 잘 되어있으면 잘 될 것이다 - 2번째까지 힘쓴다) 
데이터 분석(EDA 포함) >  (sklearn) 전처리(보간) > 학습(알고리즘 선택) > 평가(그래프 해석)
(머신러닝 덜하고 딥러닝 많이 - RF + Xgboost)

공과 & 일반 기업, 마케팅) 평가(수치가 전부) > 최적화(최적의 파라미터) - 그리드/랜덤 서치
시간 주고 컴퓨터 줄테니 다 해ㅋㅋㅋ

희소행렬 : 행렬의 크기에 비해 데이터가 적은 것(= 데이터가 있다는 1의 개수가 적다)

7장
머신러닝에서 텍스트 데이터 잘 다루지 X > 딥러닝에서 多

텍스트 데이터 > 숫자
- 우선순위 선정(핵심적)
- 필요없는 것들을 결측시킬 수 있는가?

BOW(단어 출현 빈도) : 일반적/감성분석
★ 단어 순서와 무관한 단어 개수 > 단어 빈도를 이용해서 특징을 추출하고 분류해준다
1. 단어 구분 > 토큰(가장 작은 정보량을 가진 데이터 뭉치) 분리 + 불용어 처리(ex. 의미를 가지기 힘든 것 - 분석하지 않을 것)
2. 정렬 > 토큰을 숫자로 
3. count > 누계 합산
4. 유사 단어들을 묶을 수 있다??(원점부터 단어까지 거리를 외적/내적 계산해서 별 차이 없으면 한 그룹으로 묶어도 된다)

단점
- 특정 단어가 너무 많으면 편향이 생긴다(p449 min_df 같은 걸로 편향을 줄여준다)
> 불용어 처리(강건성 ↑) > 단어 빈도 조정(빈도가 적은 단어가 정말 중요도가 덜한가?의 문제)
>> TfidfVectorizer(수-역수) : 임계값을 찾아서 좀 더 잘 분류해보자 : 전문가들의 텍스트 데이터를 분석할 때 효율적

- 하기 전에 통계 분석을 해줘야

CountVectorizer
: 비슷한 패턴의 단어끼리 분류되고 군집된다?? 헷갈린다...

NLTK 불용어 처리


텍스트 데이터 처리 시 제일 먼저 해봐야 여러 단어로 만든 BOW(n-그램) : 추천 시스템에 제일 먼저 적용하는 거?
>> n-gram 선에서 안 되면 딥 러닝으로 가라
- 가장 많이 나온 단어를 기반으로 앞뒤 단어를 본다

LSTM 알고리즘
