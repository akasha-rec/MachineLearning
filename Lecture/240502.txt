리포팅 보고서(반페이지 보고서) 연습...

복잡도 제어 > 매개변수 조정

kernel="rbf", C=10, gamma=0.1 kernel은 조정X / gamma : 민감도 (↓ = 떨어뜨리겠다 > 과적합)
경계선 위에 있는 것과 경계선에 물려있는 걸 어떻게 잘 분리해내느냐
★ 상한과 하한을 정하고 / 수만 번의 시뮬레이션을 통해 적절한 값을 찾아줄래? > 매개변수 자동 조정해주는 도구가 있다.

SVM의 전제
- 전처리를 SVM에 맞게
- 파이프라인

지도학습(y=ax+b)
1. 데이터 쌍이 필요(X, y)
(2. 규약모델-리지, 라쏘-은 다른 선형모델과 함께)
2. 결정트리 > 특징 추출(중요도 체크) >> 전처리 무관
랜덤 포레스트 > 결정트리 여러 개 > 분류/회귀에 같이 써 >> 전처리 무관

비지도학습 > 전처리 > SVM
비지도학습 (ax+b : y가 없는 대신 사람이 결정)
★ 군집(X의 특성을 공유하는 것들끼리)
전처리
1. 클리닝 > NA > 삭제(30% 이상 상회하면 삭제X) / 문자열 정리 > pandas 사용 / 문자 > 숫자로
원본 | 클리닝 > 정확성 증명(원본과 클리닝 대응관계 성립 여부)
>> 단위 맞춰줘야

2. 보정
| only 숫자 > 보간(column마다 경향성 설정)

(몇 개의 행, 몇 개의 특징) = (관측값(종속변수), 독립변수)

★ MinMaxScaler (~scaler : 범위 중요) - SVM
(min / max 값 모르고 비율 분포를 알고 싶을 때)
- 모든 데이터를 0과 1 사이의 값으로 수렴
- 비율을 100% 유지한 채 축약
- column에서 가장 큰 데이터를 100%으로 잡고(비율로서의 경향성은 유지, 원데이터는 사라져) > 원본데이터 보존해야

★ 정규화(StandardScaler)
1. 표준정규분포(평균(0)으로 환산 + 분산(1)만큼 그려서 > 단위가 사라져 : 일목요연하게 데이터 밀어넣어)
> 평균으로 회귀(비율 깨져)
min, max 값에 엄청 영향 받아 > 결국 제외
데이터를 중앙으로 모으고, 분산을 본다
중위값 > 이상치에 강해?

k-평균 군집 전처리 MinMaxScaler(간격이 큰 min/max 제외 > 보고서에 써야)
# 군집 모델을 만듭니다
kmeans = KMeans(n_clusters=3 < tsne에서 정하고 와야)
kmeans.fit(X)

t-SNE 3개 이상의 특성을 뽑는 경우 거의 X
PCA에 비해서 해석력이 월등히 좋아
fit_transform 사용
split을 안 한다
tsne가 값을 들고 있어 > min, max 들고 와서 다른 곳에 활용
ex) digits_tsne[:, 0].min(), digits_tsne[:, 0].max() 가져오기

MNEST dataset 소중하다

픽셀 : 모니터 구성하고 있는 빛나는 소자(25, 25, 25) R / G / B 3장이 겹쳐져 있는 점
컴퓨터에게 줄 때는 색을 제거해야??

from sklearn.decomposition import PCA
from sklearn.manifold import tsne
from sklearn.cluster import KMeans

PCA(주성분 분석 : ★ 직교 데이터 기반으로 고유값 어쩌고..? / 차원 축소 : 별 차이가 나지 않는 특성들은 같은 특성으로 보고 붙어라)
- 행렬 분해의 일종(대각선 행렬 구해)
- 고유값, 고유벡터
- 어떤 동일성도 공유X(직교) > 겹치지 않는 거?
- 별 차이가 안 나면 같은 걸로 볼 수 있지 않을까? > 합리적
- 수치상 관련 없을 뿐 관계가 있다?
- 정확도 정밀X(어쨌든 주의)

수치화된 정형 데이터는 머신러닝, 이미지 관련 기법은 딥 러닝으로 이양

비지도학습 돌려서 레이블 추론해서 붙이기 많이 하게 될 것