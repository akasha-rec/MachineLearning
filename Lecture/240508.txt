240508

★ 성능 향상은 평가를 동반
★ 평가할 때 그리드 서치 돌려라 + grid_search.best_params_

매개변수 숫자 조정(선택) = hyperparameter tuning
: 원래 매개변수는 컴퓨터가 결정하는데 몇몇 알고리즘은 사람이 결정하게 하는데 그 매개변수 > hyperparameter

정성적 : 정해진 게 없어서 바라보는 사람의 시각/성향에 따라 정해진다.
> 비지도학습은 평가가 불가

(K-교차검증 : 1. 2번 하는 횟수 후 평균)
교차검증 < 학습하는 과정에서 가장 많이 신경쓰는 부분 : 임의성(1. shuffle 2. split 몇 번 해보겠다.)
K 핸들링 어떻게?

score가 회귀에 모두 사용할 수 있는 건 아니다.
사용하는 알고리즘 메서드 안에 score값을 구하는 메서드가 있을 거라는 가정을 하고 사용 = cross_val_score(5~7 적당하다고 생각하는 경향)
score가 없다면? cross_val_score가 구해주지 않는다면? 메서드 안에 다른 형태의 평가 지표가 있다.
- 사용하려면 알고 있어야 하는 지식 多
- 시간이 오래 걸려
- 데이터가 정리되어 있어야

- 평균 score값을 알 수 있어서 숫자가 강하다(설명력)
- 모델의 민감도 파악
- 데이터셋이 작을 때 좀 더 효과적으로 사용할 수 있어?

from sklearn.model_selection import cross_validate #세부 데이터(실행시간) 필요할 때

교차검증을 통해 평가할 수 있다(K번 가능)
------------------------------------------------------------------------------
완전탐색 > 시간 오래 걸리고, 경우의 수 중 하나라도 빠지면 최적의 해 X

sklearn의 GridSearchCV + pipeline 양대산맥
★ 랜덤 그리드 서치 사용 ★ << GridSearchCV 옵션 달달 외워야...!!(p359)

랜덤 그리드 서치 <= 그리드 서치

그리드 서치는 비지도 불가능
그리드 서치 : 과적합 발생(검증 세트가 훈련 세트에 들어가지 않게 해야)
훈련 : 테스트 = 8 : 2 그리고 훈련 세트 안에서 다시 8 : 2

지도 vs 비지도를 할지 결정해야 > 사용하는 알고리즘이 상이

n_jobs : 바둑판 - 1

confusion_matrix : 표만 가지고 지표를 알 수 없다 < 이진 분류

정확도 / 정밀도 / 재현율 / f1 점수 > 모두 다 보는 게 원칙
>> classification_report 뽑아놓고 논의를 시작해야
분류 1. 이진( classification_report / ROC )  2. 다진(해석X > f1 점수 기준으로 잘 분류된 k개 : Top-k)

분류 > 회귀 선호 : RMSE 정확히 나와서 부담스럽다고 생각
-------------------------------------------------------------------------
파이프라인 - 결과값 예측이 안 되면 연동X ★ 반환값 ★

1. 값(숫자나 문자열 같은) 제외하면 2. 튜플 3. 딕셔너리 > 순서 고정되어있어야(반환값)

너의 프로그램의 결과가 다른 사람의 프로그램의 입력(매개변수)이 되도록 해라. > 함수는 가능한 작게 만들어야